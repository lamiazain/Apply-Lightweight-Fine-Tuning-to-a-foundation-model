# Project Introduction

Lightweight fine-tuning is one of the most important techniques for adapting foundation models, because it allows you to modify foundation models for your needs without needing substantial computational resources.

In this project, we will apply __parameter-efficient fine-tuning__ using the Hugging Face ```peft``` library.

# Project Summary:
In this project, we will bring together all of the essential components of a PyTorch + Hugging Face training and inference process. Specifically, we will:

- Load a pre-trained model and evaluate its performance
- Perform parameter-efficient fine tuning using the pre-trained model
- Perform inference using the fine-tuned model and compare its performance to the original model
# Important documentation links
Documentation Links
[Hugging Face PEFT configuration](https://huggingface.co/docs/peft/package_reference/config)
Hugging Face LoRA adapter(opens in a new tab)
Hugging Face Models save_pretrained(opens in a new tab)
Hugging Face Text Generation
  
